model:
  name: "llama"
  version: "1" # 只有triton server会用到
serverIp: "127.0.0.1"
port: 8080
requestTimeout: 1000 # 超时时间，单位为毫秒，对流式请求无效
backend: "vllm" # 模型用什么框架部署的 vllm / tgi / trt
stopWords: []
maxTokens: 16 # 要求模型一次输出多少个token，影响单条请求的速度
inputTokens: 2000 # 输入prompt的token数目大概是多长的，目前支持[100, 2000]之间的整百数，越大耗时越长
temperature: 1 # 温度，不设置的话默认是 1
stream: false # 测补全这里设置为false，测对话这里设置为true

startConcurrency: 180
endConcurrency: 5000
increment: 30
duration: 1 # 每轮持续几分钟
timeThresholds: [750, 1000, 1500, 2000, 3000] # 单位为毫秒
streamSpeedThresholds: 70 # 流式对话场景的每秒token数速度值，低于该值退出测试，取值范围(0, 100]之间的整数
saveDir: "nullxjx" # 最好使用你的企微id，方便区分

sendMsg: false
user: "nullxjx" # 你的企微英文id，填了会在群里@你